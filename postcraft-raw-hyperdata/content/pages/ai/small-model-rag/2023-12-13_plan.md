# Plan


Download model from

https://huggingface.co/TheBloke/Orca-2-7B-GGUF

danny@danny-desktop:~/AI/models$ which pip
/home/danny/.local/bin/pip
danny@danny-desktop:~/AI/models$ python3 -m venv ~/.local --system-site-packages
danny@danny-desktop:~/AI/models$ pip install huggingface-hub

start smallest

orca-2-7b.Q2_K.gguf	Q2_K	2	2.83 GB	5.33 GB	smallest, significant quality loss - not recommended for most purposes

huggingface-cli download TheBloke/Orca-2-7B-GGUF orca-2-7b.Q2_K.gguf --local-dir . --local-dir-use-symlinks False

set up for Ollama


---

https://github.com/ggerganov/llama.cpp/tree/master/examples/finetune

https://github.com/ggerganov/llama.cpp/tree/master/examples/train-text-from-scratch

https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt

- big text
---

orca-2-7b.Q4_K_M.gguf	Q4_K_M	4	4.08 GB	6.58 GB	medium, balanced quality - recommended

huggingface-cli download TheBloke/Orca-2-7B-GGUF orca-2-7b.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False



## Links

https://github.com/jmorganca/ollama

https://ollama.ai/library

https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/


https://github.com/jmorganca/ollama/blob/main/examples/python-rag-newssummary/summ.py


https://www.sbert.net/

https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

https://huggingface.co/mgoin/Orca-2-13b-pruned50-quant-ds

https://github.com/neuralmagic/deepsparse


