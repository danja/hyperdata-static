
Impedance mismatch

Preface everything with 'I could be wrong, but...'.

I don't think it's necessary to address it as the wide open space of all possible human language constructs. The training data is there as a max limit, but the LLM's structure is there as a much tighter bound. There are metrics like the number of layers, nodes, connection, context size etc. that hint at the limits of the space, but I haven't a clue how they might be exploited in practice, there must be papers...


For your consideration : possible practical tricks for improving interop between LLMs and the Web/LinkedData

@wey_gu @danbri @karpathy @kidehen @ marco @

I'll put this down a little back-to-front: first some practical bits I think might be useful (at least as a concrete route into experimentation), second some background - vague notes mostly for the benefit of my lousy memory, but here for light reading if you get *really* bored.


## Background

*A little of my personal prehistory to set the scene. I first encountered AI in 2001 : A Space Odyssey, books by Asimov & co. suggested such systems could be a reality in the distant future. Predictably I got into computers (but inexplicably early for a kid from the hills). The first AI software I ran was a Tic-tac-toe player, in BASIC, typed in from a magazine. I was fascinated, but I didn't know how to know more. Much later that balance was tipped when I took a couple of modules at university.*       
